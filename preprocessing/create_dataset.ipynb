{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import random\n",
    "import re\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_articles(start_date, end_date, limit=5000, min_views=1000):\n",
    "    articles = []\n",
    "    view_counts = {}\n",
    "    base_titles = set()\n",
    "    S = requests.Session()\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "    \n",
    "    start_date = datetime.fromisoformat(start_date.replace(\"Z\", \"+00:00\"))\n",
    "    end_date = datetime.fromisoformat(end_date.replace(\"Z\", \"+00:00\"))\n",
    "\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"allpages\",\n",
    "        \"aplimit\": \"max\",\n",
    "        \"apfilterredir\": \"nonredirects\",\n",
    "        \"apnamespace\": 0,\n",
    "        \"apfrom\": chr(random.randint(65, 90))  # Start from a random letter A-Z\n",
    "    }\n",
    "\n",
    "    while len(articles) < limit:\n",
    "        response = S.get(url=URL, params=params)\n",
    "        data = response.json()\n",
    "        allpages = data['query']['allpages']\n",
    "        for page in allpages:\n",
    "            pageid = page['pageid']\n",
    "            title = page['title']\n",
    "            \n",
    "            # Skip titles that start with numbers\n",
    "            if re.match(r'^\\d', title):\n",
    "                continue\n",
    "            \n",
    "            rev_params = {\n",
    "                \"action\": \"query\",\n",
    "                \"format\": \"json\",\n",
    "                \"prop\": \"revisions\",\n",
    "                \"rvprop\": \"timestamp\",\n",
    "                \"rvdir\": \"newer\",\n",
    "                \"rvlimit\": 1,\n",
    "                \"pageids\": pageid\n",
    "            }\n",
    "            rev_response = S.get(url=URL, params=rev_params)\n",
    "            rev_data = rev_response.json()\n",
    "            creation_date_str = rev_data['query']['pages'][str(pageid)]['revisions'][0]['timestamp']\n",
    "            creation_date = datetime.fromisoformat(creation_date_str.replace(\"Z\", \"+00:00\"))\n",
    "            \n",
    "            if start_date <= creation_date <= end_date:\n",
    "                view_params = {\n",
    "                    \"action\": \"query\",\n",
    "                    \"format\": \"json\",\n",
    "                    \"prop\": \"pageviews\",\n",
    "                    \"titles\": title\n",
    "                }\n",
    "                view_response = S.get(url=URL, params=view_params)\n",
    "                view_data = view_response.json()\n",
    "                pageviews = next(iter(view_data['query']['pages'].values())).get('pageviews', {})\n",
    "                total_views = sum(view for view in pageviews.values() if view)\n",
    "\n",
    "                if total_views >= min_views:\n",
    "                    base_title = re.sub(r'\\s\\d{4}.*', '', title)  # Remove year and subsequent text\n",
    "                    if base_title not in base_titles:\n",
    "                        articles.append(title)\n",
    "                        view_counts[title] = total_views\n",
    "                        base_titles.add(base_title)\n",
    "                    \n",
    "                    if len(articles) >= limit:\n",
    "                        break     \n",
    "        if 'continue' not in data:\n",
    "            break\n",
    "        else:\n",
    "            params['apcontinue'] = data['continue']['apcontinue']\n",
    "    \n",
    "    S.close()\n",
    "    return articles, view_counts\n",
    "\n",
    "pre_2021_articles, pre_2021_view_counts = fetch_articles(\"2020-01-01T00:00:00Z\", \"2020-12-31T23:59:59Z\", limit=10000)\n",
    "post_2024_articles, post_2024_view_counts = fetch_articles(\"2024-01-01T00:00:00Z\", \"2024-12-31T23:59:59Z\", limit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(pre_2021_articles), len(post_2024_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "def extract_texts(articles, extracts_size=5000):\n",
    "    extracts = {}\n",
    "    S = requests.Session()\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    for title in articles:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"prop\": \"extracts\",\n",
    "            \"exsentences\": 10,\n",
    "            \"exlimit\": 1,\n",
    "            \"titles\": title,\n",
    "            \"explaintext\": 1,\n",
    "            \"formatversion\": 2,\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        response = S.get(url=URL, params=params)\n",
    "        data = response.json()\n",
    "        if 'query' in data and 'pages' in data['query']:\n",
    "            page = next(iter(data['query']['pages']), None)\n",
    "            if page and 'extract' in page:\n",
    "                if len(page['extract']) > 50 and 'user' not in page['title'].lower() and not re.match(r'.+?:.+', page['title']):\n",
    "                    extracts[page['title']] = page['extract']\n",
    "        \n",
    "        if len(extracts) >= extracts_size:\n",
    "            break\n",
    "    \n",
    "    S.close()\n",
    "    return extracts\n",
    "\n",
    "pre_2021_texts = extract_texts(pre_2021_articles)\n",
    "post_2024_texts = extract_texts(post_2024_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of pre-2021 texts:\", len(pre_2021_texts))\n",
    "print(\"Number of post-2024 texts:\", len(post_2024_texts))\n",
    "print(\"Pre-2021 article view counts:\", pre_2021_view_counts)\n",
    "print(\"Post-2024 article view counts:\", post_2024_view_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to json\n",
    "save_json(pre_2021_texts, '../datasets/pre_2021_articles.json')\n",
    "save_json(post_2024_texts, '../datasets/post_2024_articles.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
